import cv2
import numpy as np
from typing import List, Dict, Optional
from facenet_pytorch import MTCNN
from config.settings import Config
import base64
from PIL import Image
import io

# EmotiEffLib ê²½ë¡œ ì„¤ì •
Config.setup_emotiefflib_path()

from emotiefflib.facial_analysis import EmotiEffLibRecognizer

def init_models(device: str = "cpu", model_name: str = "enet_b0_8_best_afew"):
    """ëª¨ë¸ ì´ˆê¸°í™”"""
    fer = EmotiEffLibRecognizer(engine="torch", model_name=model_name, device=device)
    mtcnn = MTCNN(keep_all=False, post_process=False, min_face_size=40, device=device)
    return fer, mtcnn

def recognize_faces(frame: np.ndarray, mtcnn: MTCNN) -> List[np.ndarray]:
    """ì–¼êµ´ ê°ì§€"""
    boxes, probs = mtcnn.detect(frame, landmarks=False)
    if probs is None or probs[0] is None:
        return []
    boxes = boxes[probs > 0.9]
    facial_images = []
    for box in boxes:
        x1, y1, x2, y2 = map(int, box[:4])
        facial_images.append(frame[y1:y2, x1:x2, :])
    return facial_images

def softmax_normalize_confidence(logit_scores):
    """logit ì ìˆ˜ë¥¼ 0-1 ë²”ìœ„ í™•ë¥ ë¡œ ë³€í™˜"""
    # softmax ì ìš©í•˜ì—¬ í™•ë¥ ë¡œ ë³€í™˜
    exp_scores = np.exp(logit_scores - np.max(logit_scores))  # ìˆ˜ì¹˜ì  ì•ˆì •ì„±ì„ ìœ„í•´ ìµœëŒ“ê°’ ë¹¼ê¸°
    probabilities = exp_scores / np.sum(exp_scores)
    return float(np.max(probabilities))

def analyze_image_emotion(image_data: str, device: str = "cpu") -> Optional[Dict]:
    """ì´ë¯¸ì§€ ê¸°ë°˜ ê°ì • ë¶„ì„
    
    Args:
        image_data: base64 ì¸ì½”ë”©ëœ ì´ë¯¸ì§€ ë°ì´í„°
        device: ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ (cpu/cuda)
    
    Returns:
        ê°ì • ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ ë˜ëŠ” None
    """
    try:
        fer, mtcnn = init_models(device=device)
        
        # base64 ë””ì½”ë”©
        image_bytes = base64.b64decode(image_data)
        image = Image.open(io.BytesIO(image_bytes))
        
        # PIL ì´ë¯¸ì§€ë¥¼ OpenCV í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        image_np = np.array(image)
        if len(image_np.shape) == 3:
            # RGB to BGR for OpenCV
            if image_np.shape[2] == 3:
                rgb_frame = image_np
            else:
                rgb_frame = cv2.cvtColor(image_np, cv2.COLOR_RGBA2RGB)
        else:
            print("âŒ Invalid image format")
            return None
        
        # ì–¼êµ´ ê°ì§€
        faces = recognize_faces(rgb_frame, mtcnn)
        
        if not faces:
            print("âš ï¸ No faces detected in image")
            return {
                "emotion": "Neutral",
                "confidence": 0.0,
                "face_detected": False
            }
        
        # ê°ì • ë¶„ì„ (ì²« ë²ˆì§¸ ì–¼êµ´ë§Œ ì‚¬ìš©)
        emotions, scores = fer.predict_emotions([faces[0]], logits=True)
        
        # logit ì ìˆ˜ë¥¼ 0-1 ë²”ìœ„ í™•ë¥ ë¡œ ë³€í™˜
        confidence = softmax_normalize_confidence(scores[0])
        
        result = {
            "emotion": emotions[0],
            "confidence": round(confidence, 3),
            "face_detected": True
        }
        
        print(f"âœ… Image emotion analysis: {result['emotion']} (confidence: {result['confidence']} = {round(confidence*100, 1)}%)")
        return result
        
    except Exception as e:
        print(f"âŒ Image emotion analysis error: {str(e)}")
        return {
            "emotion": "Neutral",
            "confidence": 0.0,
            "face_detected": False,
            "error": str(e)
        }

def analyze_video_emotion(video_path: str, device: str = "cpu", frame_interval: int = 10) -> List[Dict]:
    """ë™ì˜ìƒ ê°ì • ë¶„ì„"""
    fer, mtcnn = init_models(device=device)

    results = []
    cap = cv2.VideoCapture(video_path)
    fps = cap.get(cv2.CAP_PROP_FPS)
    frame_idx = 0

    print(f"ğŸ” Analyzing video: {video_path}")

    while cap.isOpened():
        success, frame = cap.read()
        if not success:
            break

        if frame_idx % frame_interval == 0:
            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            faces = recognize_faces(rgb_frame, mtcnn)

            if faces:
                emotions, scores = fer.predict_emotions(faces, logits=True)
                
                # logit ì ìˆ˜ë¥¼ 0-1 ë²”ìœ„ í™•ë¥ ë¡œ ë³€í™˜
                confidence = softmax_normalize_confidence(scores[0])
                
                results.append({
                    "frame": frame_idx,
                    "time_sec": round(frame_idx / fps, 2),
                    "emotion": emotions[0],
                    "confidence": round(confidence, 3)
                })

        frame_idx += 1

    cap.release()
    print(f"âœ… Finished analyzing. Total analyzed frames: {len(results)}")
    return results 